{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "elder-johns",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data='fra.txt'\n",
    "num_samples=10000\n",
    "#to collect input data and target data\n",
    "input_texts=[]\n",
    "target_texts=[]\n",
    "input_char=set()\n",
    "target_char=set()\n",
    "with open(data,'r',encoding='utf-8') as f:\n",
    "    lines=f.read().split('\\n')\n",
    "for line in lines[:min(num_samples,len(lines)-1)]:\n",
    "    input_text,target_text,nothing=line.split('\\t')\n",
    "    #we use 'tab' as the start sequence char for the targets, and \\n as end sequence char\n",
    "    target_text='\\t'+target_text+'\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_char:\n",
    "            input_char.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_char:\n",
    "            target_char.add(char)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ambient-insurance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ',\n",
       " '!',\n",
       " '\"',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '5',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " '?',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'Y',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " 'é'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "oriental-tomorrow",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\t',\n",
       " '\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '5',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " '?',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'Y',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " '\\xa0',\n",
       " '«',\n",
       " '»',\n",
       " 'À',\n",
       " 'Ç',\n",
       " 'É',\n",
       " 'Ê',\n",
       " 'à',\n",
       " 'â',\n",
       " 'ç',\n",
       " 'è',\n",
       " 'é',\n",
       " 'ê',\n",
       " 'î',\n",
       " 'ï',\n",
       " 'ô',\n",
       " 'ù',\n",
       " 'û',\n",
       " 'œ',\n",
       " '\\u2009',\n",
       " '’',\n",
       " '\\u202f'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "legitimate-territory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples:  10000\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'num_encoder_token' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-44087632c646>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"number of samples: \"\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_texts\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"number of unique input tokens : \"\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mnum_encoder_token\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"number of unique output tokens : \"\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mnum_decoder_token\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Maximum sequence length of input : \"\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mmax_encoder_sequence_length\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Maximum sequence length for target : \"\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mmax_decoder_sequence_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'num_encoder_token' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "significant-stevens",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_char=sorted(list(input_char))\n",
    "target_char=sorted(list(target_char))\n",
    "num_encoder_token=len(input_char)\n",
    "num_decoder_token=len(target_char)\n",
    "max_encoder_sequence_length=max([len(text) for text in input_texts])\n",
    "max_decoder_sequence_length=max([len(text) for text in target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "amino-transport",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples:  10000\n",
      "number of unique input tokens :  71\n",
      "number of unique output tokens :  93\n",
      "Maximum sequence length of input :  15\n",
      "Maximum sequence length for target :  59\n"
     ]
    }
   ],
   "source": [
    "print(\"number of samples: \" , len(input_texts) )\n",
    "print(\"number of unique input tokens : \" , num_encoder_token )\n",
    "print(\"number of unique output tokens : \" , num_decoder_token )\n",
    "print(\"Maximum sequence length of input : \" , max_encoder_sequence_length )\n",
    "print(\"Maximum sequence length for target : \" , max_decoder_sequence_length) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "catholic-colonial",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " '!',\n",
       " '\"',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '5',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " '?',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'Y',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " 'é']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "single-guinea",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index= dict(\n",
    "    [(char,i) for i,char in enumerate(input_char)])\n",
    "target_token_index=dict([(char,i) for i,char in enumerate(target_char)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "decimal-builder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " '!': 1,\n",
       " '\"': 2,\n",
       " '$': 3,\n",
       " '%': 4,\n",
       " '&': 5,\n",
       " \"'\": 6,\n",
       " ',': 7,\n",
       " '-': 8,\n",
       " '.': 9,\n",
       " '0': 10,\n",
       " '1': 11,\n",
       " '2': 12,\n",
       " '3': 13,\n",
       " '5': 14,\n",
       " '7': 15,\n",
       " '8': 16,\n",
       " '9': 17,\n",
       " ':': 18,\n",
       " '?': 19,\n",
       " 'A': 20,\n",
       " 'B': 21,\n",
       " 'C': 22,\n",
       " 'D': 23,\n",
       " 'E': 24,\n",
       " 'F': 25,\n",
       " 'G': 26,\n",
       " 'H': 27,\n",
       " 'I': 28,\n",
       " 'J': 29,\n",
       " 'K': 30,\n",
       " 'L': 31,\n",
       " 'M': 32,\n",
       " 'N': 33,\n",
       " 'O': 34,\n",
       " 'P': 35,\n",
       " 'Q': 36,\n",
       " 'R': 37,\n",
       " 'S': 38,\n",
       " 'T': 39,\n",
       " 'U': 40,\n",
       " 'V': 41,\n",
       " 'W': 42,\n",
       " 'Y': 43,\n",
       " 'a': 44,\n",
       " 'b': 45,\n",
       " 'c': 46,\n",
       " 'd': 47,\n",
       " 'e': 48,\n",
       " 'f': 49,\n",
       " 'g': 50,\n",
       " 'h': 51,\n",
       " 'i': 52,\n",
       " 'j': 53,\n",
       " 'k': 54,\n",
       " 'l': 55,\n",
       " 'm': 56,\n",
       " 'n': 57,\n",
       " 'o': 58,\n",
       " 'p': 59,\n",
       " 'q': 60,\n",
       " 'r': 61,\n",
       " 's': 62,\n",
       " 't': 63,\n",
       " 'u': 64,\n",
       " 'v': 65,\n",
       " 'w': 66,\n",
       " 'x': 67,\n",
       " 'y': 68,\n",
       " 'z': 69,\n",
       " 'é': 70}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "loaded-multimedia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\t': 0,\n",
       " '\\n': 1,\n",
       " ' ': 2,\n",
       " '!': 3,\n",
       " '$': 4,\n",
       " '%': 5,\n",
       " '&': 6,\n",
       " \"'\": 7,\n",
       " '(': 8,\n",
       " ')': 9,\n",
       " ',': 10,\n",
       " '-': 11,\n",
       " '.': 12,\n",
       " '0': 13,\n",
       " '1': 14,\n",
       " '2': 15,\n",
       " '3': 16,\n",
       " '5': 17,\n",
       " '8': 18,\n",
       " '9': 19,\n",
       " ':': 20,\n",
       " '?': 21,\n",
       " 'A': 22,\n",
       " 'B': 23,\n",
       " 'C': 24,\n",
       " 'D': 25,\n",
       " 'E': 26,\n",
       " 'F': 27,\n",
       " 'G': 28,\n",
       " 'H': 29,\n",
       " 'I': 30,\n",
       " 'J': 31,\n",
       " 'K': 32,\n",
       " 'L': 33,\n",
       " 'M': 34,\n",
       " 'N': 35,\n",
       " 'O': 36,\n",
       " 'P': 37,\n",
       " 'Q': 38,\n",
       " 'R': 39,\n",
       " 'S': 40,\n",
       " 'T': 41,\n",
       " 'U': 42,\n",
       " 'V': 43,\n",
       " 'Y': 44,\n",
       " 'a': 45,\n",
       " 'b': 46,\n",
       " 'c': 47,\n",
       " 'd': 48,\n",
       " 'e': 49,\n",
       " 'f': 50,\n",
       " 'g': 51,\n",
       " 'h': 52,\n",
       " 'i': 53,\n",
       " 'j': 54,\n",
       " 'k': 55,\n",
       " 'l': 56,\n",
       " 'm': 57,\n",
       " 'n': 58,\n",
       " 'o': 59,\n",
       " 'p': 60,\n",
       " 'q': 61,\n",
       " 'r': 62,\n",
       " 's': 63,\n",
       " 't': 64,\n",
       " 'u': 65,\n",
       " 'v': 66,\n",
       " 'w': 67,\n",
       " 'x': 68,\n",
       " 'y': 69,\n",
       " 'z': 70,\n",
       " '\\xa0': 71,\n",
       " '«': 72,\n",
       " '»': 73,\n",
       " 'À': 74,\n",
       " 'Ç': 75,\n",
       " 'É': 76,\n",
       " 'Ê': 77,\n",
       " 'à': 78,\n",
       " 'â': 79,\n",
       " 'ç': 80,\n",
       " 'è': 81,\n",
       " 'é': 82,\n",
       " 'ê': 83,\n",
       " 'î': 84,\n",
       " 'ï': 85,\n",
       " 'ô': 86,\n",
       " 'ù': 87,\n",
       " 'û': 88,\n",
       " 'œ': 89,\n",
       " '\\u2009': 90,\n",
       " '’': 91,\n",
       " '\\u202f': 92}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "internal-silence",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data=np.zeros((len(input_texts),max_encoder_sequence_length,num_encoder_token),dtype='float32')\n",
    "decoder_input_data=np.zeros((len(input_texts),max_decoder_sequence_length,num_decoder_token),dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dirty-entry",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_target_data=np.zeros((len(input_texts),max_decoder_sequence_length,num_decoder_token),dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closing-switzerland",
   "metadata": {},
   "source": [
    "# one hot codding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "changed-prevention",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,(input_text,target_text) in enumerate(zip(input_texts,target_texts)):\n",
    "    for t,char in enumerate(input_text):\n",
    "        encoder_input_data[i,t,input_token_index[char]]=1\n",
    "    encoder_input_data[i,t+1:,input_token_index[' ']]=1\n",
    "    for t,char in enumerate(target_text):\n",
    "        #decoder target data is ahead of decoder input data by one time step\n",
    "        decoder_input_data[i,t,target_token_index[char]]=1\n",
    "        if t>0:\n",
    "            #decoder will ahead by one time step and it will not include the start char\n",
    "            decoder_target_data[i,t-1,target_token_index[char]]=1\n",
    "    decoder_input_data[i,t+1:,target_token_index[' ']]=1\n",
    "    decoder_target_data[i,t:,target_token_index[' ']]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "parliamentary-generator",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 71)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "divided-given",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input,LSTM,Dense\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "finnish-decision",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim=256 #latient dimensionality of encoding space\n",
    "encoder_inputs=Input(shape=(None,num_encoder_token))\n",
    "encoder=LSTM(latent_dim,return_state=True)\n",
    "encoder_output,state_h,state_c=encoder(encoder_inputs)\n",
    "\n",
    "#here we discard the encoder outputs and only take states\n",
    "encoder_states=[state_h,state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "administrative-conclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs=Input(shape=(None,num_decoder_token))\n",
    "#we will not use return states in the training model but we will use them in interface\n",
    "decoder_lstm=LSTM(latent_dim,return_sequences=True,return_state=True)\n",
    "decoder_outputs, _, _=decoder_lstm(decoder_inputs,initial_state=encoder_states)\n",
    "decoder_dense=Dense(num_decoder_token,activation='softmax')\n",
    "decoder_outputs=decoder_dense(decoder_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "canadian-madrid",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define mmodel which will turn encoder input to decoder input to decoder target data\n",
    "\n",
    "model=Model([encoder_inputs,decoder_inputs],decoder_outputs)\n",
    "model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "sapphire-scratch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "55/55 [==============================] - 13s 244ms/step - loss: 0.7922 - accuracy: 0.7882 - val_loss: 0.8308 - val_accuracy: 0.7750\n",
      "Epoch 2/10\n",
      "55/55 [==============================] - 14s 264ms/step - loss: 0.7086 - accuracy: 0.8064 - val_loss: 0.7675 - val_accuracy: 0.7818\n",
      "Epoch 3/10\n",
      "55/55 [==============================] - 14s 264ms/step - loss: 0.6556 - accuracy: 0.8169 - val_loss: 0.7083 - val_accuracy: 0.7971\n",
      "Epoch 4/10\n",
      "55/55 [==============================] - 14s 251ms/step - loss: 0.5960 - accuracy: 0.8281 - val_loss: 0.6703 - val_accuracy: 0.8046\n",
      "Epoch 5/10\n",
      "55/55 [==============================] - 15s 264ms/step - loss: 0.5659 - accuracy: 0.8351 - val_loss: 0.6417 - val_accuracy: 0.8134\n",
      "Epoch 6/10\n",
      "55/55 [==============================] - 15s 268ms/step - loss: 0.5366 - accuracy: 0.8428 - val_loss: 0.6189 - val_accuracy: 0.8195\n",
      "Epoch 7/10\n",
      "55/55 [==============================] - 15s 270ms/step - loss: 0.5132 - accuracy: 0.8496 - val_loss: 0.6020 - val_accuracy: 0.8232\n",
      "Epoch 8/10\n",
      "55/55 [==============================] - 15s 271ms/step - loss: 0.4924 - accuracy: 0.8551 - val_loss: 0.5780 - val_accuracy: 0.8301\n",
      "Epoch 9/10\n",
      "55/55 [==============================] - 15s 278ms/step - loss: 0.4753 - accuracy: 0.8601 - val_loss: 0.5755 - val_accuracy: 0.8282\n",
      "Epoch 10/10\n",
      "55/55 [==============================] - 15s 282ms/step - loss: 0.4606 - accuracy: 0.8638 - val_loss: 0.5521 - val_accuracy: 0.8389\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d98797bb20>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([encoder_input_data,decoder_input_data],decoder_target_data,batch_size=128,\n",
    "         epochs=10,validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "distinguished-reggae",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "latent_dim=256\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_token))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = target_char[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_sequence_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "international-auction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001D98FFD2EE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001D907BD1820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'num_decoder_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-26f9cc82b910>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0minput_seq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoder_input_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mseq_index\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mseq_index\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mdecoded_sentence\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecode_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'input : '\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0minput_texts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mseq_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-d6529d895229>\u001b[0m in \u001b[0;36mdecode_sequence\u001b[1;34m(input_seq)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;31m# Update the target sequence (of length 1).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0mtarget_seq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_decoder_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m         \u001b[0mtarget_seq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msampled_token_index\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'num_decoder_tokens' is not defined"
     ]
    }
   ],
   "source": [
    "for seq_index in range(100):\n",
    "    input_seq=encoder_input_data[seq_index:seq_index+1]\n",
    "    \n",
    "    decoded_sentence=decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('input : ' ,input_texts[seq_index])\n",
    "    print('output : ', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atomic-greene",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
